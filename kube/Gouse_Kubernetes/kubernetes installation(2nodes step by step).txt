											****************************************************************************************************************************************************************
                                             Summary: This Document provide how to install kubernetes cluster(2nodes for test environment-no HA) step by step.The Os version of machines is Centos 7.7
																				###Please Consider new version of kubernetes might reflect below steps###
																				   At the end of this Document we gonna have kubernetes cluster v1.16.0
											Master node in this document: Hostname=km1 IP=192.168.30.10
											Worker node is this document: Hostname=kn1 IP=192.168.30.30 
											****************************************************************************************************************************************************************

											
Step 1)prerequisite kubernetes installation

1.1)Turn off Selinux(ALL HOSTS)
																					
																		****************************************************************************************
																		containers need to access the host filesystem and this can be done by pod network.right 
																		now there is no strong support from SElinux for this purpose and it restrict those needs
																		so we need to disable it.
																		****************************************************************************************
setenforce 0
cp /etc/selinux/config /etc/selinux/config_$(date +%Y_%m_%d)_backup
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config


1.2)Disable Swap(ALL HOSTS)
																		****************************************************************************************
																		Kubernetes requires to disable the swap at the OS level.From distributed system 
																		prespective that is designed to operate at scale using SWAP is pointless at all.If swap
																		os enabled on any hosts of kubernetes cluster the kubelet will faile
																		****************************************************************************************
								
cp /etc/fstab /etc/fstab_$(date +%Y_%m_%d)_backup
sed -i -r 's/^.*swap*/#&/' /etc/fstab
swapoff -a


1.3)Iptables instead of Firewalld (Optional-ALLHOSTS)
																		****************************************************************************************
																		Since i have no expreince working with firewalld i replace back the iptables.
																		it is recommended to keep iptables off during installation and add the rules at the end of
																		test.The below RULES are for Kubernetes component communication Please feel free to add
																		other rules on demand
																		****************************************************************************************
																																
																		
																		
systemctl stop firewalld;systemctl disable firewalld;systemctl mask firewalld
yum install iptables iptables-services
cp /etc/sysconfig/iptables /etc/sysconfig/iptables_$(date +%Y_%m_%d)_backup


																						 KubernetesIPTABLES RULES FOR MASTER NODE
																		---------------------------------------------------------------------
																		Protocol 	Direction 		portRange 			  Purpose 
	 																	  TCP        Inbound          6443*        		 API server
																		  TCP	     Inbound        2379-2380    		etcd client API
																		  TCP		 Inbound          10250      	     kubelet API
																		  TCP        Inbound          10251      		Kube-scheduler
																		  TCP		 Inbound          10252      		Kube-controller-manager
																		---------------------------------------------------------------------
vim /etc/sysconfig/iptables
-A INPUT -p tcp --dport 6443 -m comment --comment "kuber API server" -j ACCEPT
-A INPUT -p tcp --match multiport --dports 2379:2380 -m comment --comment "kuber etcd server client API" -j ACCEPT
-A INPUT -p tcp --dport 10250 -m comment --comment "kuber Kubelet API" -j ACCEPT
-A INPUT -p tcp --dport 10251 -m comment --comment "kuber kube-scheduler" -j ACCEPT
-A INPUT -p tcp --dport 10252 -m comment --comment "kuber controller-manager" -j ACCEPT



																 					 Kubernetes IPTABLES RULES FOR WORKER NODE
																		---------------------------------------------------------------------
																		Protocol 	Direction 		portRange 			  Purpose 
	 																	  TCP        Inbound          10250        		 kubelet API
																		  TCP	     Inbound        30000-32767    		NodePort Services
vim /etc/sysconfig/iptables												---------------------------------------------------------------------
-A INPUT -p tcp  --dport 10250 -m comment --comment "kuber Kubelet API" -j ACCEPT
-A INPUT -p tcp --match multiport --dports 30000:32767 -m comment --comment "kuber NodePort Services" -j ACCEPT



1.4)Specify all hostname and Their ip address in /etc/hosts



######################################################################################################################################################################################################################################
2) Install and Config Container Runtime interface(CRI)-Docker(ALL HOSTS)
																		
2.1)install required packages
yum install yum-utils device-mapper-persistent-data lvm2
yum-config-manager  --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum update
yum install docker-ce-18.06.2.ce  --> docker-ce version 18.06.2 is recommended by kubernetes official site
			
															****************************************************************************************
															A single cgroup manager will simplify the view of what resources are being allocated and 
															will by default have a more consistent view of the available and in-use resources. When 
															we have two managers we end up with two views of those resources.We have seen cases in 
															the field where nodes that are configured to use cgroupfs for the kubelet and Docker, and
															systemd for the rest of the processes running on the node becomes unstable under resource 
															pressure.Changing the settings such that your container runtime and kubelet use systemd 
															as the cgroup driver stabilized the system	---> native.cgroupdriver=systemd
															*****************************************************************************************
2.2)Config docker daemon to set systemd as default cgroup manager
																		
mkdir /etc/docker

cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

systemctl daemon-reload
systemctl start docker
systemctl status docker
systemctl enable docker
docker ps
######################################################################################################################################################################################################################################
Step 3) Installing kubeadm, kubelet and kubectl on all hosts(ALL HOSTS)

3.1) Create the kubernetes repo
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

3.2) install Packages
													****************************************************************************************
													kubeadm: Need for cluster bootstrap
															 
													kubelet: Work as a agent and runs on all hosts.mainely responsible for starting pods and
															 container
															 
													kubectl: The command interface that talk to your cluster
													****************************************************************************************
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

3.3)
													****************************************************************************************
													If the kubrenetes traffic bypass the iptables we might get into trouble for that we must
													make sure br_netfilter modules is running and set net.bridge.bridge-nf-call-iptables to 1
													Also make sure ip forwarding is enabled													
													****************************************************************************************
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

lsmod | grep br_netfilter   --> if no output then run modprobe br_netfilter


3.4)Restart kubelet

systemctl enable kubelet
systemctl restart kubelet

########################################################################################################################################################################################################################################
4)Creating a single control-plane cluster(Only in MASTER NODE)

4.1) Creating single control-plane cluster with kubeadm
	
												 *******************************************************************************************
												--pod-network-cidr:specify pods cidr network,since we use flannel as network pluggins for
												  pods communication then we set this value to 10.244.0.0/16													
													
												--apiserver-advertise-address: this parameter is used to make the master addvertise him self
												  to others in specific network interface.Please feel free to add your own IP address
												*******************************************************************************************

kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.2.172  --> This step might take time to complete as it does some pre-checks and pull some images.If everything is successfull it will provide 
																							   you some bellow command to run to create config file and a command with a token for worker nodes to join the cluster

4.2)Command which kubeadm init provide to create a config file
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config																							   

4.3)Command which kubeadm init provide to run on worker node to join the cluster(Only in worker node)
kubeadm join 192.168.30.10:6443 --token jtbsmt.qtsktksur2z6fjf9 --discovery-token-ca-cert-hash sha256:ae4bbfe30bf2d0ff652dc8e05b5a8424a8692cfea72cb705eb5c9753d2119e00

######################################################################################################################################################################################################################################
5)Check the cluster from MASTER NODE

																	***************************************************
																	Please consider that the STATUS of all nodes must be
																	Ready and the STATUS of all pods must be Running
																	if both STATUS is something else please read step 6
																	***************************************************
[root@km1 ~]# kubectl get nodes -o wide
NAME              STATUS   ROLES    AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
km1.localdomain   Ready    master   37h   v1.16.0   192.168.30.10   <none>        CentOS Linux 7 (Core)   3.10.0-1062.1.1.el7.x86_64   docker://18.6.2
kn1.localdomain   Ready    <none>   30h   v1.16.0   192.168.30.30   <none>        CentOS Linux 7 (Core)   3.10.0-1062.1.1.el7.x86_64   docker://18.6.2

[root@km1 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-gzt8m                  1/1     Running   4          25h
kube-system   coredns-5644d7b6d9-kc66s                  1/1     Running   4          25h
kube-system   etcd-km1.localdomain                      1/1     Running   9          37h
kube-system   kube-apiserver-km1.localdomain            1/1     Running   9          37h
kube-system   kube-controller-manager-km1.localdomain   1/1     Running   10         37h
kube-system   kube-flannel-ds-amd64-828s9               1/1     Running   4          23h
kube-system   kube-flannel-ds-amd64-8k57s               1/1     Running   3          23h
kube-system   kube-proxy-57t7j                          1/1     Running   6          30h
kube-system   kube-proxy-t8j74                          1/1     Running   9          37h
kube-system   kube-scheduler-km1.localdomain            1/1     Running   10         37h

[root@km1 ~]# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE    IP              NODE              NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-gzt8m                  1/1     Running   4          25h    10.244.0.11     km1.localdomain   <none>           <none>
kube-system   coredns-5644d7b6d9-kc66s                  1/1     Running   4          25h    10.244.0.10     km1.localdomain   <none>           <none>
kube-system   etcd-km1.localdomain                      1/1     Running   9          37h    192.168.30.10   km1.localdomain   <none>           <none>
kube-system   kube-apiserver-km1.localdomain            1/1     Running   9          37h    192.168.30.10   km1.localdomain   <none>           <none>
kube-system   kube-controller-manager-km1.localdomain   1/1     Running   10         37h    192.168.30.10   km1.localdomain   <none>           <none>
kube-system   kube-flannel-ds-amd64-828s9               1/1     Running   4          23h    192.168.30.10   km1.localdomain   <none>           <none>
kube-system   kube-flannel-ds-amd64-8k57s               1/1     Running   3          23h    192.168.30.30   kn1.localdomain   <none>           <none>
kube-system   kube-proxy-57t7j                          1/1     Running   6          30h    192.168.30.30   kn1.localdomain   <none>           <none>
kube-system   kube-proxy-t8j74                          1/1     Running   9          37h    192.168.30.10   km1.localdomain   <none>           <none>
kube-system   kube-scheduler-km1.localdomain            1/1     Running   10         37h    192.168.30.10   km1.localdomain   <none>           <none>

######################################################################################################################################################################################################################################
6)
														*****************************************************************************
														If the STATUS of nodes is NotReady and the STATUS of coredns is pending then
														we might have variety of problem.The bellow steps might help you to solve the
														problem but does not guarantee anything.
														****************************************************************************
6.1) Check the Selinux and Swap is off in all nodes.check what is written in step one


6.2) Check if cniVersion is missing in /etc/cni/net.d/10-flannel.conflis

														****************************************************************************
														the /etc/cni/net.d/10-flannel.conflis is the file that need to be checked.
														This file is created by kube-flannel-cfg configmap and according to our 
														research it comes with a BUG which is does not specify the cniVersion in it.
														in order to make sure that that CNIversion is not specified please cat the 
														/etc/cni/net.d/10-flannel.conflis.
														****************************************************************************
													

cat /etc/cni/net.d/10-flannel.conflist

[root@km1 ~]# cat /etc/cni/net.d/10-flannel.conflist
{
  "name": "cbr0",
  "cniVersion": "0.2.0",    ------> if this is missing then we need to edit kube-flannel-cfg
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}
					
6.3) Edit kube-flannel-cfg

[root@km1 ~]# kubectl get configmap --all-namespaces
NAMESPACE     NAME                                 DATA   AGE
kube-public   cluster-info                         1      37h
kube-system   coredns                              1      37h
kube-system   extension-apiserver-authentication   6      37h
kube-system   kube-flannel-cfg                     2      24h     --> this need to be edit
kube-system   kube-proxy                           2      37h
kube-system   kubeadm-config                       2      37h
kube-system   kubelet-config-1.16                  1      37h


[root@km1 ~]# kubectl edit configmap/kube-flannel-cfg  --namespace=kube-system   --> this command takes you to vim environment

 .
 .
 apiVersion: v1
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.2.0",   --> add this and save then exit
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
 .
 . 

6.4) Restart the machines and problem might be solved
															***********************************************************************************
															If the problem does not solve please use journalctl -f -u kubelet to follow the
																				logs and find out the problem you can also use 
																		kubectl describe pods/my-podname --namespace=my-namespace
															to find a lot of good information about pods status and their problems.
															After the problem is solved you can start iptables(optional) to check if kubernetes
															does not get into problems.
															***********************************************************************************
															
Contact me on linkedin  :https://www.linkedin.com/in/mohammad-ali-kamalian-7a3a72124/															
															
															